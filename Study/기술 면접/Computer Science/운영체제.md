# 운영체제란?

## 운영체제
운영체제란 컴퓨터 시스템의 핵심 소프트웨어로, 컴퓨터 하드웨어와 응용프로그램 간의 상호작용을 관리하고 제어하는 역할을 한다.
<br>

## 운영체제 목적 
사용자와 하드웨어 사이의 인터페이스를 제공해 효율적으로 응용 프로그램이 동작하도록 지원하고, 시스템 자원을 효율적으로 관리하여 응용 프로그램이 원할하게 동작할 수 있도록 한다.
<br>

## 운영체제 역할

### 자원 관리
CPU, 메모리, 디스크 공간, 네트워크 등과 같은 컴퓨터 자원을 효율적으로 관리합니다.

### CPU 스케줄링과 프로세스 관리
CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생성과 삭제, 자원 할당 및 반환을 관리한다. 또한, 프로세스 간 통신(IPC), 동기화 및 데드락 감지 및 해결과 같은 기능을 제공하여, 멀티태스킹 환경에서 프로세스가 원활히 실행될 수 있도록 한다.

### 메모리 관리
한정된 메모리를 어떤 프로세스에 얼만큼 할당해야 하는지 관리한다.

### 파일 시스템 관리
파일 생성, 읽기, 쓰기 등 파일 시스템 관련 작업을 관리한다.

### I/O 디바이스 관리
I/O 디바이스와 컴퓨터 간에 데이터를 주고 받는 것을 관리한다.
<br>

## 운영체제 구조

### 커널 (Kernel)
커널은 용어 자체로도 '핵심의' 로 운영체제에서 중요한 역할이다. 
커널은 전반적으로 **프로세스**를 뜻한다.
프로세스 관리, 메모리 관리, 저장공간 관리, 연결된 장치 관리 등 컴퓨터에 속한 모든 자원을 관리하는 역할을 한다.

### 인터페이스 (Interface)
인터페이스는 사용자의 명령을 컴퓨터에 전달하고 결과를 알려주는 소통 역할을 한다.
인터페이스 종류는 GUI (Graphical User Interface), CLI(Command Line Interface) 가 대표적이다.
GUI는 그래픽 요소를 사용하여 소통하게끔 하여 사용자 편의성을 가진 인터페이스이고, CLI는 명령어로 소통하는 인터페이스이다.

### 시스템 콜 (System Call)
시스템 콜은 운영체제가 커널에 접근하기 위한 인터페이스이다.
사용자나 프로그램이 직접적으로 컴퓨터 자원에 접근하는 것을 막고 커널을 보호하기 위해 만든 코드 집합이다. 운영체제는 시스템 콜을 통해서만 커널에 접근 하도록 디자인되었다.

### 드라이버 (Driver)
드라이버는 프린터 키보드 등 하드웨어 장치와 운영체제 간의 통신을 가능하게 하는 소프트웨어이다. 
<br>

> 운영체제는 컴퓨터 하드웨어와 응용프로그램 간의 인터페이스 역할을 하는 컴퓨터 시스템 핵심 소프트웨어입니다. 
> 운영체제는 자원 관리, 프로세스 관리, 메모리 관리, 파일 시스템, 입출력 관리 등의 역할을 합니다.

---
# 프로세스 메모리 구조

- 메모리 공간 종류 4가지 : 코드 영역, 데이터 영역, 힙 영역, 스택 영역

![[Pasted image 20240312160044.png]]

### 코드 영역 (Code / Text)
실행할 프로그램의 코드가 CPU가 해석 가능한 기계어로 저장된다. CPU는 이 영역에서 명령어를 하나씩 가져와 처리한다.

### 데이터 영역 (Data)
전역 변수와 static 변수가 저장된다.

### 스택 영역 (Stack)
지역 변수, 매개변수, 실행되는 함수에 의해 늘어나거나 줄어드는 메모리 영역이다. 함수가 호출될 때마다 환경 등 특정 정보가 저장되는 독립적인 공간이다.
스택은 함수 호출과 함께 할당되며, 호출 종료 시 소멸된다.
(stack 영역 초과 시 stack overflow 에러 발생)
### 힙 영역 (Heap)
생성자, 인스턴스와 같은 동적으로 할당되는 데이터들을 위해 존재하는 공간이다. 
사용자가 직접 관리하는 영역으로 메모리 공간이 동적으로 할당되고 해제된다. 

위 그림을 보면 스택, 힙은 화살표로 되어 있는데, 프로세스가 실행되는 동안 크기가 늘었다 줄어들었다하는 동적 공간이기 때문에 화살표로 표현되었다.

> 메모리에는 크게 코드, 데이터, 힙, 스택 영역이 있습니다.
> `코드`는 소스코드가 들어가는 공간이고, `데이터`는 전역 변수, static 변수가 할당되는 부분입니다.
> `힙`은 사용자가 직접 관리하는 영역으로 데이터가 동적으로 할당되는 공간이고,
> `스택`은 함수의 호출 정보, 지역 변수, 매개변수들에 의해 늘어나거나 줄어드는 메모리 영역입니다. 함수 호출 시 할당되며, 종료 시 소멸됩니다.

---
# Process, Thread 비교

## 정의

- 프로세스 : 운영체제로부터 자원을 할당받은 작업의 단위. 실행중인 프로그램
- 스레드 : 프로세스 내에서 동시에 진행되는 실행 단위 (멀티 스레드)

## 메모리 영역

- 프로세스 : 각각의 독립된 메모리 영역(코드, 데이터, 힙, 스택)을 할당 받는다.
- 스레드 : 프로세스 내에서 각각 스택만 따로 할당 받고, 코드, 데이터, 힙은 스레드 간 공유 된다.

## Context Switching

- 프로세스 컨텍스트 스위칭 : 커널 모드 전환 + CPU 상태 교체 + 메모리 영역 상태 교체 (MMU 수정 + TLB 캐시 비우기)
- 스레드 컨텍스트 스위칭 : 커널 모드 전환 + CPU 상태 교체

  

## 차이점

| 차이점           | 프로세스          | 스레드                   |
| ---------------- | ----------------- | ------------------------ |
| 정의             | 실행중인 프로그램 | 프로세스 실행 단위       |
| 생성 / 종료 시간 | 많은 시간 소요    | 적은 시간 소요           |
| 컨텍스트 스위칭  | 많은 시간 소요    | 적은 시간 소요           |
| 데이터 공유      | IPC 사용          | 공유 메모리 사용         |
| 자원 소모량      | 많음              | 적음                     |
| 메모리 독립성    | 독립적            | 스택만 독립적, 이외 공유 |
|                  |                   |                          |
> `프로세스`는 실행중인 프로그램이고, 
> `스레드`는 프로세스 안에서 실행되는 흐름 단위입니다.
> `프로세스`는 메모리와 CPU를 프로세스마다 할당 받아서 사용하는데, 
> `스레드`는 프로세스 안에서 다른 스레드와 메모리와CPU를 공유해서 사용합니다. 스레드는 스택 메모리만 각각 할당 받습니다.

---
# Thread에서 독립적인 Stack 영역이 필요한 이유


스레드는 프로세스가 할당 받은 자원을 이용하는 실행 단위이다. 
스레드끼리 프로세스의 자원을 공유하면서 프로세스의 실행 흐름의 일부가 되기 때문에,동시 작업이 가능하다.

스레드는 프로세스의 4가지 메모리 영역 중 Stack만 할당받아 복사하고, 나머지 메모리 영역을 프로세스 내의 다른 스레드들과 함께 공유한다. (각각의 스레드는 별도의 Stack 을 가지고 있다.)

스택은 함수 호출 시 전달되는 인자, 되돌아갈 주소값, 함수 내에서 선언하는 변수 등을 저장하는 메모리 공간으로, 독립적인 함수 호출이 가능하다. 이는 독립적인 실행 흐름이 추가 된다는 말이다.

결과적으로 스레드에서 독립적인 Stack 메모리 영역이 필요한 이유는 하나의 프로세스를 다수의 실행 단위인 스레드로 구분하여 자원을 공유하고, 자원의 생성과 관리의 중복성을 최소화하여 수행 능력을 올리기 위해서다.

> 스레드에 독립적인 Stack 영역이 필요한 이유는 하나의 프로세스를 다수의 실행 단위인 스레드로 구분하여 자원을 공유하고, **자원의 생성과 관리의 중복성을 최소화**하여 수행 능력을 올리기 위해서입니다.


---
# 멀티 프로세스와 멀티 스레드 비교

## 멀티 프로세스

멀티 프로세스는 운영체제에서 하나의 응용 프로그램에 대해 동시에 여러 개의 프로세스를 실행할 수 있게 하는 기술을 말한다.
보통 하나의 프로그램은 하나의 프로세스가 메모리에 생성되지만 부가 기능을 위해 여러 개의 프로세스를 생성하는 것이다.

### 장점

#### 프로그램 안전성

멀티 프로세스는 각 프로세스가 독립적인 메모리 공간을 가지므로, 한 프로세스가 비정상적으로 종료되어도 다른 프로세스에 영향을 주지 않는다. 따라서 프로그램 전체의 안전성을 확보할 수 있다.

#### 프로그램 병렬성

멀티 프로세스와 여러 개의 CPU 코어를 활용하여, 다중 CPU 시스템에서 각 프로세스를 병렬적으로 실행하여 성능을 향상 시킬 수 있다.

이 장점은 멀티 스레드의 장점이기도 하다.

#### 시스템 확장성

멀티 프로세스는 각 프로세스가 독립적이므로, 새로운 기능이나 모듈을 추가하거나 수정할 때 다른 프로세스에 영향을 주지 않는다. 따라서 시스템 규모를 쉽게 확장할 수 있다.

### 단점

#### Context Switching Overhead

멀티 태스킹 구성의 핵심 기술인 컨텍스트 스위칭 과정에서 성능 저하가 올 수있다. 
특히 프로세스를 컨텍스트 스위칭하면 `CPU는 다음 프로세스의 정보를 불러오기 위해 메모리를 검색하고, CPU 캐시 메모리를 초기화하며, 프로세스 상태를 저장하고, 불러올 데이터를 준비해야하기 때문에` 빈번한 컨텍스트 스위칭 작업으로 인해 오버헤드가 발생할 수 있다.
반면에, 스레드를 컨텍스트 스위칭하면 프로세스 스위칭보다 가벼워 훨씬 빠르고 좋다.

따라서, 멀티 프로세스 환경에서는 Context Switching Overhead를 최소화하는 방법이 중요하다.
프로세스 수를 적정하게 유지하거나, I/O 바운드 작업이 많은 프로세스와 CPU 바운드 작업이 많은 프로세스를 분리하여 관리하고, CPU 캐시를 효율적으로 활용하는 등의 방법을 고려해 봐야 한다.

#### 자원 공유 비효율성

멀티 프로세스는 각 프로세스가 독립적인 메모리 공간을 가지므로, 메모리 사용량이 증가하게 된다.
각 프로세스 간 자원 공유가 필요할 경우 어렵고 복잡한 통신 기법인 IPC(Inter-Process Communication) 를 사용해야 한다.
IPC는 실행 중인 프로세스 간에 정보를 주고받는 메커니즘을 말한다. 이를 위해 파이프, 소켓, 메세지 큐 등 다양한 방법이 사용된다. 그런데 IPC 자체로 오버헤드가 발생한다. 예를 들어, 파이프나 소켓과 같은 IPC 기법은 데이터를 복사하거나 버퍼링하는 과정에서 성능 저하가 발생할 수 있기 때문이다. 또한 코드의 복잡도를 증가시킨다.

<br>

## 멀티 스레드

스레드는 하나의 프로세스 내에 있는 실행 단위이다. 멀티 스레드는 하나의 프로세스 안에 여러 개의 스레드가 있어, 두 가지 이상의 동작을 동시에 처리하도록 하는 것이다.

멀티 프로세스와의 차이점은 웹 브라우저로 예시를 들어보면 웹 브라우저에서 여러 탭이나 여러 창이 각각 프로세스로 멀티 프로세스이고, 멀티 스레드는 단일 탭 내에서 브라우저 이벤트 루프, 네트워크 처리, I/O 및 기타 작업을 관리하고 처리하는 데 사용된다.

### 장점

많은 운영체제들이 멀티 프로세싱을 지원하지만 멀티 스레드를 기본으로 하고 있다. 

#### 프로세스보다 가벼움

스레드는 프로세스 내에서 생성되기 때문에 실행 환경 설정 작업이 매우 간단하여 생성 및 종료가 빠르다.
또한, 코드, 데이터, 힙 영역을 서로 공유하기 때문에 데이터 용량이 프로세스보다 작다.

#### 자원의 효율성

멀티 스레드는 하나의 프로세스 내에서 생성되기 때문에, 공유 메모리에 대해 스레드 간 자원 공유가 가능하다. 

#### Context Switching 비용 감소

스레드도 컨텍스트 스위칭 오버헤드가 존재하지만, 프로세스 컨텍스트 스위칭에 비하면 훨씬 낮다.
프로세스 컨텍스트 스위칭 시 CPU 캐시를 모두 초기화하고, 새로운 프로세스 정보를 적재해야 하므로 높은 비용이 든다. 하지만 스레드는 공유 자원을 제외한 스레드 정보(stack, register) 만 교체하면 되므로 상대적으로 낮다.

#### 응답 시간 단축

앞선 장점들로 인해 응답 시간이 빠르다.

### 단점

#### 안정성 문제

멀티 스레드는 하나의 스레드에서 문제가 발생하면 다른 스레드들도 영향을 받아 전체 프로그램이 종료될 수 있다.
하지만 이 문제는 적절한 예외 처리, 에러 발생 시 새로운 스레드를 생성하거나 스레드 풀에서 잔여 스레드를 가져오는 등의 방법으로 해결할 수 있다.

#### 동기화로 인한 성능 저하

멀티 스레드는 여러 개의 스레드가  공유 자원에 동시 접근할 수 있기 때문에 동기화 문제가 발생할 수 있다. 따라서 스레드 간 동기화(syncronized)는 데이터 접근을 제어하기 위한 필수적인 기술이다.

동기화 작업은 여러 스레드 접근을 제한하는 것이기 때문에 병목 현상이 일어나 성능 저하될 가능성이 높다.

이를 해결하기 위해 임계 영역에 대해 뮤텍스, 세마포어 방식을 활용한다.

#### 데드락 (교착 상태)

데드락은 다수의 프로세스나 스레드가 서로의 자원을 점유하고 무한정 자원을 기다리는 상황에서 발생하는 교착상태를 말한다.

데드락 방지를 위해 상호배제, 점유와 대기, 비선점, 순환대기 등의 알고리즘을 사용하면 된다.

#### Context Switching Overhead

컨텍스트 스위칭 오버헤드 비용 자체를 무시할 수는 없다. 스레드 수가 많을 수록 컨텍스트 스위칭이 많이 발생하고, 성능 저하로 이어질 수 있다.

>**멀티 프로세스**는 여러 개의 독립적인 프로세스가 동시에 실행되는 것을 말합니다. 각 프로세스는 독립된 메모리 공간(코드, 데이터, 힙, 스택 등)을 할당받으며, 다른 프로세스의 자원에 접근할 수 없습니다. 프로세스 간 통신을 위해서는 Inter Process Communication(IPC) 기법(예: 파이프, 소켓, 공유 메모리 등)을 사용해야 합니다. 멀티 프로세스는 하나의 프로세스에 문제가 발생해도 다른 프로세스에는 영향을 미치지 않는 격리성을 가지고 있지만, 프로세스 간 컨텍스트 스위칭이나 IPC로 인한 오버헤드가 크다는 단점이 있습니다.
> **멀티 스레드**는 하나의 프로세스 내에서 여러 개의 스레드가 동시에 실행되는 것을 말합니다. 모든 스레드는 부모 프로세스의 자원(메모리 공간)을 공유하며, 각각 별도의 스택을 가지고 있어 독립적인 실행 흐름을 가집니다. 멀티 스레드는 스레드 간 데이터 공유가 용이하고, 컨텍스트 스위칭 비용이 상대적으로 낮아서 효율적인 자원 사용과 빠른 실행 속도를 가질 수 있습니다. 하지만, 스레드 간 자원 공유로 인해 데이터 일관성 및 동기화 문제가 발생할 수 있으며, 한 스레드에서 발생한 문제가 전체 프로세스에 영향을 미칠 수 있다는 단점이 있습니다.

---
# 문맥 교환 (Context Switch)

문맥 교환은 멀티태스킹 운영체제에서 CPU가 이전 작업(프로세스 or 스레드)에서 다른 작업으로 전환할 때 발생하는 과정을 말한다. 이 과정에서 운영체제는 현재 실행 중인 작업의 상태(문맥)을 저장하고, 다음에 실행할 작업의 상태를 메모리에 불러와 CPU에 적재한다. 

이때 한 프로세스의 문맥은 프로세스 제어 블록(PCB)에 기록되어 있다.

## 문맥 교환 시점

### 멀티 태스킹

멀티 태스킹은 다수의 프로세스가 하나의 CPU 자원을 나눠 사용하는 것이다.
실행 가능한 프로세스들이 운영체제의 스케줄러에 의해 조금씩 번갈아 수행되는데, 프로세스가 **CPU를 할당 받을 때** 문맥 교환이 일어난다.
매우 빠른 속도로 처리되기 때문에 동시에 처리되는 것처럼 느껴진다.

### 인터럽트 처리

인터럽트는 컴퓨터 시스템에서 예외 상황이 발생했을 때 CPU에게 알려 처리할 수 있도록 하는 것이다.
**인터럽트가 발생했을 때** 문맥 교환이 일어난다.

- I/O request : 입출력 요청
- time slice expried : CPU 사용시간 만료
- fork a child : 자식 프로세스 생성
- wait for an interrupt : 인터럽트 처리 대기

### 사용자 및 커널 모드 전환

운영체제에서 사용자 모드와 커널 모드 사이의 전환이 필요할 때 필수는 아니지만 운영체제에 따라 문맥 교환이 발생한다.


## 문맥 교환이 일어나는 과정

1. 요청 발생 : 인터럽트 또는 트랩에 의한 요청이 발생
2. PCB에 저장 : 운영체제는 현재 실행중인 프로세스의 정보를 PCB에 저장
3. CPU 할당 : 운영체제는 다음 프로세스의 정보를 PCB에서 가져와 CPU를 할당

<br>

---
# Process Control Block (PCB)

프로세스 제어 블록은 특정한 프로세스를 관리할 필요가 있는 정보를 포함하는 운영체제 커널의 자료구조이다. 

운영체제가 프로세스 스케줄링을 위해 프로세스에 관한 모든 정보를 가지고 있는 데이터베이스를 PCB라 한다.

운영체제에서 프로세스는 PCB로 나타내어지며, 프로세스에 대한 중요한 정보를 가지고 있는 자료이다. 각 프로세스가 생성될 때마다 고유의 PCB가 생성되고, 프로세스가 완료되면 PCB는 제거된다.

프로세스는 CPU가 처리하던 작업의 내용들을 자신의 PCB에 저장하고, 다음에 다시 CPU를 점유하여 작업을 수행할 때 PCB로 부터 해당 정보들을 CPU에 넘겨와서 계속해서 하던 작업을 진행할 수 있게 된다.



---
# 경쟁 상태(Race Condition)와 임계 영역(Critical Section)

## 경쟁 상태 (Race Condition)
경쟁 상태란 공유 자원에 대해 여러 프로세스가 동시에 접근할 때, 결과 값에 영향을 줄 수 있는 상태

### 경쟁 상태가 발생하는 경우

> 유저모드: 사용자가 접근할 수 있는 영역을 제한적으로 두고, 프로그램의 자원에 함부로 접근하지 못하는 모드. 코드를 작성하고, 프로세스를 실행하는 행동을 할 수 있다.

> 커널모드: 모든 자원(CPU, 메모리 등) 접근, 명령 가능한 모드

- 커널 코드 실행 중 인터럽트가 발생하는 경우
	- 커널 모드에서 데이터를 로드하여 작업 도중 인터럽트가 발생하여 같은 데이터를 조작하는 경우에 발생할 수 있다.
	- 커널이 가진 전역 변수는 모든 프로세스의 공유물이므로 경쟁상태의 가능성이 있다.
	- 해결 방법 : 커널 모드에서 작업을 수행하는 동안 인터럽트를 Disable하여 CPU 제어권을 가지지 못하도록 한다.
- 프로세스가 시스템 콜을 하여 커널 모드로 진입해서 작업 도중 문맥 교환이 발생하는 경우
	- 프로세스1이 커널 모드에서 작업 중 시간 초과로 CPU 제어권이 프로세스2로 넘어가 같은 데이터를 조작하는 경우를 말한다.
	- 해결 방법 : 커널 모드 작업 시 CPU 제어권이 넘어가지 않도록 한다.
- 멀티 프로세서에서 공유 메모리 내의 커널 데이터에 접근할 경우
	- 멀티 프로세스 환경에서 2개의 CPU가 동시에 커널 내부의 공유 데이터에 접근하여 조작하는 경우 발생할 수 있다.
	- 해결 방법 : 커널 내부에 있는 공유 데이터에 접근할 때마다, 그 데이터에 대해 Lock / Unlock 함으로써 해결할 수 있다.

## 임계 영역 (Critical Section)

임계 영역은 운영체제에서 여러 프로세스가 데이터를 공유하면서 수행될 때 각 프로세스에서 공유 자원에 접근하는 프로그램 코드 부분을 의미한다.

프로세스 간에 공유 자원을 접근하는 데 있어서 문제가 발생하지 않도록 **공유 자원의 독점을 보장해 주어야 하는 영역**이다.

### 임계 영역 문제를 해결하기 위한 3가지 조건

- 상호배제 (Mutual Exclusion)
	- 한 프로세스가 자신의 임계 영역이면 다른 프로세스들은 임계 영역에 진입할 수 없다.
- 진행 (Progress)
	- 아무도 임계 영역에 없다면 진입하고자 하는 프로세스를 진입하게 해줘야 한다.
	- 다음에 어떤 프로세스가 임계 영역에 진입해야 하는지 유한한 시간에 결정되어야 한다.
- 유한 대기 (Bounded Waiting)
	- 프로세스가 임계 영역에 진입하기 위해 무한정으로 기다리는 현상이 발생해서는 안된다.

임계 영역은 뮤텍스, 세마포어, 모니터 등의 동기화 기법을 사용하여 관리할 수있다. 이러한 동기화 기법들은 동시에 임계 영역에 접근하는 것을 막아서 데이터 일관성과 시스템 안정성을 보장한다.

> 경쟁 상태란 여러 프로세스 또는 여러 스레드가 공유 데이터를 동시에 접근할 때 공유 데이터에 대한 실행 순서에 따라 실행 결과가 달라지는 상황을 말합니다.
> 경쟁 상태는 상호배제를 안했을 때 나타나는 현상입니다.
> 임계 영역은 둘 이상의 프로세스에 의해 동시에 접근하면 안되는 공유 자원에 접근하는 영역(코드 영역) 즉, 프로그램에서 공유 데이터를 이용하는 부분입니다.

---
# 경쟁 상태를 막기 위한 방법

## 뮤텍스 (Mutex)

뮤텍스는 공유된 자원의 데이터를 여러 스레드가 접근하는 것을 막는 방법이다.
즉, 임계 영역(각 프로세스에서 공유 데이터를 엑세스하는 프로그램 코드 부분)을 가진 스레드들의 런타임이 서로 겹치지 않게 단독으로 실행되게 하는 기술이다.

다중 프로세스들이 공유 리소스에 대한 접근을 조율하기 위해 Lock / Unlock을 사용하는데, 상호배제를 함으로써 두 스레드가 동시에 사용할 수 없다는 뜻이다.

## 세마포어 (Semaphore)

공유된 자원의 데이터를 여러 프로세스가 접근하는 것을 막는 것이다. 또한, 리소스의 상태를 나타내는 간단한 카운터라고 할 수 있는데, 일반적으로 비교적 긴 시간을 확보하는 리소스에 대해 이용하게 되며, 리소스를 경쟁적으로 사용하는 다중 프로세스에서 행동을 조정하거나 동기화 시키는 기술이다.

하나의 스레드만 들어가게 할 수 있고 여러 개의 스레드가 들어가게 할 수도 있다.

Q1. 세마포어는 경쟁상태를 어떻게 해결하나?
Q2. 스핀락이 무엇?
Q3. busy waiting, sleep lock?
Q4. 뮤텍스 세마포어 차이 / 
A4. 통제 해제 방식에서 차이가 있다 뮤텍스 락방식 

우선순위 역전 - 우선순위가 낮은 애가 먼저 선점하는 경우 / 동기화  해결 방법 - 우선순위 올림 우선순위 상속

## 모니터

모니터는 뮤텍스와 조건 변수를 함께 사용하여 임계 영역에 대한 접근을 관리하는 고수준 동기화 방법이다. 모니터 내의 모든 메소드는 자동적으로 임계 영역이 되며, 동시에 하나의 스레드만이 모니터 내의 메소드를 실행할 수 있다.

> 경쟁 상태를 막기 위한 방법은 대표적으로 뮤텍스와 세마포어 동기화 기법이 있습니다.
> 뮤텍스는 공유 자원에 대해 여러 스레드가 접근하지 못하도록 막는 방법입니다. 임계 영역에 하나의 스레드만 접근하도록 하여 경쟁 상태를 방지할 수 있습니다. 세마포어는 동시에 자원에 접근할 수 있는 스레드나 프로세스의 최대 수를 제한하는 동기화 기법입니다.

---
# 임계 영역, 경쟁 상태, 뮤텍스, 세마포어 정리

> 임계 영역은 둘 이상의 프로세스에 의해 동시에 접근하면 안되는 공유 자원에 접근하는 영역입니다. 이를 어겼을 때 경쟁 상태가 발생하는데, 경쟁 상태가 발생하지 않도록 하는 방법은 뮤텍스와 세마포어가 대표적입니다. 뮤텍스는 공유 자원에 대해 여러 스레드가 접근하지 못하도록 막는 방법입니다. 임계 영역에 하나의 스레드만 접근하도록 하여 경쟁 상태를 방지할 수 있습니다. 세마포어는 동시에 자원에 접근할 수 있는 스레드나 프로세스의 최대 수를 제한하는 동기화 기법입니다.

---
# 교착 상태(DeadLock)와 발생 조건 4가지

## 교착 상태 (DeadLock)

데드락은 두 개 이상의 프로세스나 스레드가 서로 자원을 얻지 못해서 다음 처리를 하지 못하고 무한정 다음 자원을 기다리는 상태를 말한다.
시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생한다.

## 데드락 발생 조건

### 1. 상호 배제 (Mutual exclusion)

자원은 한번에 한 프로세스만 사용할 수 있다.

### 2. 점유 대기 (Hold and Wait)

최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 존재해야 한다.

### 3. 비선점 (No Preemption)

다른 프로세스에 할당된 자원은 사용이 끝날 때까지 강제로 뺏을 수 없다.

### 4. 순환 대기 (Circular Wait)

프로세스의 집합에서 순환 형태로 자원을 대기하고 있어야 한다.

대기하는 프로세스의 집합 (P0, P1, ... Pn)에서  P0은 P1이 점유한 자원을 기다리고, P1 -> P2 ... Pn -> P0 이런 식으로 순환적 대기 상태가 존재한다.


> 데드락은 프로세스가 자원을 얻지 못해 다음 작업을 못하는 상태이다.
> 예를 들어, P1, P2가 각각 자원 A와 B를 얻어야 되는데, P1은 A를 P2는 B를 가지고 있어서 서로 무한정 대기하는 상태를 데드락이라고 한다.
> 데드락은 상호 배제, 점유 대기, 비선점, 순환 대기 네가지가 동시에 성립될 때 발생한다.

---
# 교착 상태 해결 방법

데드락을 방지하는 방법은 데드락 발생 조건 4가지 중 하나 이상을 제거하거나 회피하는 데 중점을 둔다.

1. 자원을 할당할 때 애초에 조건이 성립되지 않도록 설계한다 (예방)
2. 교착 상태 가능성이 없을 때만 자원이 할당되며, 프로세스당 요청할 자원들의 최대치를 통해 자원 할당 가능 여부를 파악하는 은행원 알고리즘을 사용한다. (회피)
3. 교착 상태가 발생하면 사이클이 있는지 찾아보고 이에 관련된 프로세스를 한 개씩 지운다. (탐지 & 회복)
4. 교착 상태는 매우 드물게 일어나기 때문에 이를 처리하는 비용이 더 커서 교착 상태가 발생하면 사용자가 작업을 종료한다. 현대 운영체제는 이 방법을 채택했다. 예를 들어 프로세스 실행 중 '응답 없음' 발생

## 예방 (Prevention)

교착 상태 발생 조건 중 하나를 제거하면서 해결한다. (자원 낭비 매우 심함)

- 상호 배제 부정
	- 여러 프로세스가 공유 자원을 사용
- 점유 대기 부정
	- 프로세스 실행 전 모든 자원을 할당
- 비선점 부정
	- 자원 점유 중인 프로세스가 다른 자원을 요구할 때 가진 자원을 반납
- 순환 대기 부정
	- 자원에 고유 번호를 할당한 후 순서대로 자원을 요구
	- 따라서 프로세스의 속도를 떨어뜨리고 자원의 접근을 불필요하게 거부하여 비효율적이다.

### 점유 대기

1. 프로세스가 작업을 시작하기 전에 필요한 모든 자원을 요청하여 할당받도록 하는 것이다.
2. 프로세스가 전혀 자원을 갖지 않은 상태에서만 자원 요청을 허용하고, 프로세스가 자원을 추가 요청하려면 가지고 있는 자원을 모두 해제해야 요청을 승인한다.
- 예를 들어 DVD 드라이버에서 디스크로 파일 하나를 복사해서 정렬 후 프린터로 출력한다고 했을 때, 
- 방법 1은 나중에 필요한 프린터도 프로세스 초기에 점유해서 다른 프로세스가 프린터를 사용하지 못하게 하는 것이다.
- 방법 2는 DVD 드라이버에서 디스크로 파일 하나를 복사한 후 DVD 드라이버와 디스크 파일을 모두 해제하고, 다시 디스크 파일과 프린터를 점유한 후 프린터를 한 후 모두 해제한다.

### 비선점

- 어떤 프로세스가 자원을 가지고 실행 중 다른 자원을 요청 했는데 그 자원을 점유할 수 없다면 지금까지 가지고 있던 자원을 모두 해제하도록 한다. 그 후, 프로세스는 원래 자원과 새로 원하는 자원을 함께 요청해야 한다.
- 한 프로세스에서 다른 프로세스가 점유한 자원을 원하면, 운영체제는 다른 프로세스가 점유한 자원을 강제로 반납시키고 그것을 원하는 프로세스에게 할당할 수 있다. 이 방법은 프로세스들이 서로 다른 우선순위를 가지고 있을 때 우선순위를 기준으로 선점하여 교착 상태를 예방할 수 있다. 
- 비선점의 단점은 프로세서 레지스터나 기억장치 레지스터와 같이 쉽게 저장되고 이후 복원이 쉬운 자원에는 사용 가능하지만 프린터와 같은 자원은 활용이 어렵다.


## 회피 (Avoidance)

교착 상태 발생 시 회피하는 방법

- 은행원 알고리즘 (Banker's Algorithm)
	- 프로세스가 자원을 요구할 때 시스템은 자원을 할당한 후에도 안정 상태로 남아있게 되는지를 사전에 검사하는 알고리즘
	- 안정상태를 유지할 수 있으면 자원을 할당하고, 다른 프로세스가 자원을 해제할 때까지 대기한다.
	- 안전상태: 시스템이 교착상태를 일으키지 않으면서 각 프로세스가 요구한 최대 요구량만큼 필요한 자원을 할당해 줄 수 있는 상태
	- 항상 안전 상태를 유지할 수 있다는 장점이 있으나 최대 자원 요구량을 미리 알아야 하고 항상 불안전 상태를 방지해야 하므로 자원 이용도가 낮다.

은행원 알고리즘은 교착 상태를 회피하는 알고리즘으로 총 자원의 양과 현재 할당한 자원의 양을 기준으로 안정 또는 불안정 상태로 나누고 안정 상태로 가도록 자원을 할당하는 알고리즘이다.

리소스 요청을 허락해줬을 때 데드락 발생 가능성이 있으면 리소스를 할당해도 안전할 때까지 계속 요청을 거절하는 알고리즘


## 탐지 & 회복 (Detection and Recovery)

교착 상태가 되도록 허용한 후 회복시키는 방법

### 탐지

자원 할당 그래프를 통해 교착 상태를 탐지한다.
데드락 탐지 알고리즘을 사용하여 시스템을 주기적으로 검사하고 데드락 존재 여부를 확인한다.

### 회복

데드락을 해결하기 위해 하나 이상의 프로세스를 종료하거나, 할당된 자원을 해제시켜 회복시키는 방법이다.
프로세스 종료 방법으로는 교착 상태의 프로세스를 모두 1중지하거나, 교착 상태가 제거될 때까지 하나씩 프로세스를 중지하는 방법이 있다.
자원 선점 방법으로는 교착 상태의 프로세스가 점유하고 있는 자원을 선점해 해당 프로세스를 정지시키고 다른 프로세스에게 할당하거나 우선순위가 낮은 프로세스나 수행 횟수가 적은 프로세스 위주로 프로세스 자원을 선점하는 방법이 있다.




---
# 기아 상태

기아 상태는 특정 프로세스의 우선 순위가 낮아서 원하는 자원을 항당받지 못하는 상태를 말한다.

### 교착 상태와 기아 상태 차이

- 교착 상태 : 프로세스가 자원을 얻지 못해 다음 처리를 못하는 상태
- 기아 상태 : 프로세스가 원하는 자원을 계속 할당 받지 못하는 상태

즉, 교착 상태는 여러 프로세스가 동일한 자원 점유를 원할 때 발생하고 기아 상태는 여러 프로세스가 자원을 점유하기 위해 경쟁할 때 특정 프로세스는 계속 자원 할당을 받지 못하는 것이다.

### 해결 방법

- 프로세스 우선 순위를 수시로 변경하여 각 프로세스가 높은 우선 순위를 가질 기회를 준다. (우선순위 올림, 상속)
- 오래 기다린 프로세스 우선 순위를 높여준다.
- 우선 순위가 아닌 요청 순서대로 처리하는 FIFO 기반 요청 큐를 사용한다.





---
# CPU 스케줄링? 스케줄링 종류

## 프로세스 상태

![[Pasted image 20240318175342.png]]
### New
생성 되었지만 운영체제에 의해 수행 가능한 프로세스 풀로 진입이 아직 허용되지 않은 프로세스

### Ready
기회가 주어지면 수행될 준비가 되어있는 프로세스

### Running
현재 수행 중인 프로세스

### Blocked (블록/대기)
I/O 작업(입출력 연산) 완료 등과 같은 이벤트가 발생할 때까지 수행될 수 없는 프로세스

### Exit
프로세스 수행이 종료되거나, 어떤 이유로 중단되어 프로세스 풀에서 방출된 프로세스

### 디스패처 (Dispatcher) 란?

CPU를 한 프로세스로부터 다른 프로세스로 교체해주는 프로그램이다.

쉽게말해, 프로그램이 실행되다 중단되고, 다음 프로그램을 실행하게 해주는 프로그램이라고 볼 수 있다.

Ready -> Running 단계에서 스케줄러 또는 디스패처가 준비 상태에 있는 프로세스 중 하나를 선택해 실행하게 한다.

## CPU 스케줄링 

> 운영체제가 프로세스에 합리적으로 CPU 자원을 할당하는 과정을 의미한다.
> Ready 상태의 프로세스 중에서 어떤 프로세스에게 CPU를 할당할지 결정하는 것을 CPU 스케줄링이라고 한다.

Dispatcher는 CPU 제어권을 CPU 스케줄러에 의해 선택된 프로세스에게 넘기는데, 이를 Context Switch(문맥 교환)라고 한다.

### 목적

CPU 스케줄링 목적은 모든 프로세스가 적당히, 공평하게, 효율적으로 자원을 할당받는 것이다. 하지만 다양한 상황에서 만족 시키기는 쉽지 않다. 그래서 자연스럽게 이를 위한 고려사항들을 떠올리고, 그에 맞는 알고리즘들이 설계되었다.

- 공평성 : 프로세스에게 자원을 배분하는 과정은 공평해야한다.
- 효율성 : 시스템 자원이 쉬는 시간이 없어야 한다. (CPU를 놀게 하면 안됨)
- 안정성 : 중요 프로세스들은 우선권을 주어야 한다. 또한 프로세스가 증가해도 안정적으로 돌아가야 한다.
- 반응 시간 보장 : 응답이 없는 경우 사용자는 시스템이 멈춘 것으로 가정하기 때문에 적절한 시간 안에 프로세스의 요구에 반응해야 한다.
- 무한 연기 방지 : 특정 프로세스의 작업이 무한히 연기되어서는 안된다.


OS의 Scheduler란 컴퓨터의 자원을 활용해서 일을 할당하는 개념으로, 크게 세가지로 나뉜다.

- Long-term scheduler: 디스크에서 메모리로 프로세스를 스케줄링
- Medium-term scheduler: 메모리와 디스크 사이의 스왑 관리
- Short-term scheduler: CPU와 메모리 사이의 스케줄링

우리가 주목할 것은 Short-term scheduler이다.

메모리에 있는 프로세스를 CPU로 보내는 것이 Short-term scheduler이다.

![[Pasted image 20240319211724.png]]

 해야할 일을 정하는 것(어떤 프로세스를 선택할지) 까지가 Scheduler의 일, 실제로 cpu를 Process에 할당하는 것이 Dispathcer의 일이다.
 
![[Pasted image 20240319211749.png]]

그래서 넓은 의미의 Scheduling은 Scheduler와 Dispatcher 기능을 동시에 말한다.


# CPU 스케줄링 종류

## 선점형 스케줄링(preemptive scheduling)

한 프로세스가 CPU를 사용 중일 때, 더 높은 우선순위를 가진 프로세스가 도착하면 운영체제가 현재 실행중인 프로세스를 중지시키고, 새로운 프로세스에 CPU를 할당할 수 있다.

- 우선 순위가 높은 프로세스를 빠르게 처리해야할 경우 유용하다.
- 선점이 일어날 경우, 오버헤드가 발생하며 처리시간을 예측하기 힘들다.
- 잦은 Context Switching이 일어날 경우 오버헤드가 많이 발생한다.

비선점 스케줄링은 선점 스케줄링과 반대이다.
선점 스케줄링은 I/O 요청, I/O 응답, Interrupt 발생, 작업 완료 등의 상황에서 스케줄링이 일어날 수 있다. (선점이 일어남)
비선점 스케줄링의 경우 프로세스가 작업이 완료되는 시점에만 스케줄링이 일어난다.

### SRTF (Shortest Remaining Time First) 스케줄링

SJF의 선점형 방식이다. 먼저 온 프로세스가 CPU를 할당받고 있더라도 남은 처리 시간이 뒤에 온 프로세스의 처리 시간보다 길면 CPU를 빼앗긴다. (짧은 프로세스 먼저)

하지만 기본적으로 선점형 방식이기 때문에 잦은 문맥 교환이 일어나고 그에 따른 오버헤드가 커진다. 그리고 기아 현상이 더 심각하게 발생할 수 있다.(처리시간이 긴 프로세스는 할당 못받음)


### 라운드 로빈 (Round-Robin) 스케줄링

라운드 로빈은 프로세스에게 각각 동일한 CPU 할당 시간(타임 슬라이스, quantum)을 부여해서 이 시간 동안만 CPU를 이용하게 한다. 만약 할당 시간동안 처리를 다 하지 못하면 CPU를 빼앗고 다음 프로세스에게 넘긴다. 빼앗긴 프로세스는 준비 큐의 맨 뒤로 간다. 따라서 선점형 방식이다.
따로 CPU 처리 시간을 계산하지 않아도 돼서 선점형 방식의 가장 단순하고 대표적인 방법이다. 우선 순위도 없기 때문에 매우 공평하다.

모든 프로세스가 최초 응답 시간을 빠르게 보장받을 수 있다는 큰 장점을 가지게 된다. 자연히 콘보이 효과가 줄어든다.

> 콘보이 효과란, CPU를 매우 오래 사용하는 프로세스가 도착하게 되면, 다른 프로세스가 CPU를 사용하는데 기다리는 대기 시간이 매우 커지는 현상을 말한다.

라운드 로빈에서 가장 중요한 부분은 타임 슬라이스 크기 결정이다.

타임슬라이크가 크면 처리 시간이 긴 프로세스에 의해 CPU 효율성이 떨어질 수 있다. 너무 크면 FCFS와 다를게 없다.
반대로 작을 경우 여러 프로그램이 동시에 실행되는 효과를 볼 수 있지만, 너무 작으면 잦은 문맥 교환으로 오버헤드가 상당히 커진다.

### 다단계 큐(Multi-level Queue) 스케줄링

다단계 큐 스케줄링은 우선순위에 따라 준비 큐를 여러 개 사용하는 방식이다. 당연히 우선 순위가 높은 큐에 먼저 CPU가 할당되어 큐에 속한 모든 프로세스가 처리돼야 다음 우선순위 큐가 실행될 수 있다. 
그리고 한 번 우선순위가 매겨져 준비 큐에 들어가면 이 우선순위는 바뀌지 않는다.

프로세스를 우선순위에 따라 그룹화하고, 각 그룹에 대해 서로 다른 스케줄링을 적용할 수 있다.


![[Pasted image 20240319011637.png]]

각 큐는 독립적인 스케줄링 알고리즘을 가질 수 있는데, 보통 전면 프로세스들이 속해있는 큐는 우선순위가 높고, 라운드 로빈 스케줄링을 사용해 타임 슬라이스를 작게한다.

후면 프로세스에는 사용자와의 상호작용이 없으므로 가장 간단한 FCFS 방식으로 처리한다. 
보통 총 CPU 시간이 전면 프로세스 처리 80%, 후면 프로세스 처리 20%가 할당된다.

**다단계 큐 알고리즘 문제**
- 기아 현상
- 공평성 문제

> [!note] 전면 프로세스, 후면 프로세스?
> - 전면 프로세스: GUI를 사용하는 운영체제에서 화면의 맨 앞에 높인 프로세스이다. 입력과 출력을 사용하는 프로세스이며, **사용자와 상호작용이 가능하여 상호작용 프로세스**라고도 한다.
> - 후면 프로세스: 사용자와의 상호작용이 없는 프로세스이다. 압축 프로그램처럼 사용자의 입력 없이 작동하기 때문에 **일괄 작업 프로세스**라고도 한다.
> - 전면 프로세스는 사용자의 요구에 즉각 반응해야 하지만 후면 프로세스는 상호작용이 없다. 따라서 전면 프로세스의 우선순위가 후면 프로세스보다 높다. 그만큼 후면 프로세스는 전면 프로세스보다 CPU를 할당받을 확률이 적다.

### 다단계 피드백 큐 스케줄링

다단계 큐 공평성 문제를 완화하기 위해 신분 하락이 가능한 알고리즘이다. 
이 알고리즘은 우선순위가 변동되기 때문에 큐 사이의 이동이 가능하다.

한 번 CPU를 할당 받은 프로세스는 우선순위가 조금 낮아진다. 따라서 더 낮은 큐로 이동하게 된다.

이를 더 보완하기 위해 우선순위가 높은 큐보다 낮은 큐에 타임 슬라이스 크기를 크게 준다.
어렵게 얻은 CPU를 더 오래 사용하게 해주기 위함이다.



## 비선점형 스케줄링(non-preemptive scheduling)

- 이미 할당된 CPU를 다른 프로세스가 강제로 빼앗아 사용할 수 없는 기법이다.
- 프로세스가 CPU를 할당받으면, 해당 프로세스가 자발적으로 CPU를 방출하거나(IO 입출력 발생할 때) 실행이 종료될 때까지 다른 프로세스가 CPU를 선점할 수 없다. 
- 필요한 Context Switching 만 일어나기 때문에 오버헤드가 상대적으로 적지만 프로세스의 배치에 따라 효율성 차이가 많이 난다.

### SJF(Shortest Jop First) 스케줄링

준비 큐에 있는 프로세스 중 실행 시간이 가장 짧은 작업부터 CPU를 할당하는 비선점형 방식이다. 늦게 도착하더라도 CPU 처리 시간이 앞에 대기중이 프로세스보다 짧으면 먼저 CPU를 할당 받을 수 있기 때문에 콘보이 효과를 완화할 수 있다.

SJF 스케줄링은 FCFS 보다 효율성이 매우 높지만, 공평성에 어긋난다.
처리 시간이 긴 프로세스의 경우 짧은 프로세스가 계속 들어오면 기아 현상이 발생할 수 있다.

### HRN(Highest Response Ratio Next) 스케줄링

긴 작업과 짧은 작업 간의 지나친 불평등을 어느정도 보완한 기법이다.
수행 시간의 길이와 대기 시간을 모두 고려해 우선순위를 정한다.
SJF 스케줄링에 Aging 기법을 합친 비선점형 알고리즘이다.

Aging은 나이를 먹는다는 의미 그대로 기아현상을 해결하기 위해 대기 시간이 길어지면 우선순위를 높여주는 방법이다.

SJF와 마찬가지로 실행 시간이 적은 프로세스의 우선순위가 높지만, 대기 시간이 너무 길어지면 실행 시간이 길더라도 CPU를 할당받을 수 있다. 하지만 공평성이 말끔히 해결되지는 않는다.

### FCFS 스케줄링 (First Come First Serve Scheduling)

CPU를 먼저 요청한 프로세스가 먼저 배정 받는 스케줄링 방법이다. (선입선출)
프로세스가 준비 큐에 도착한 순서에 따라 CPU를 할당한다.
프로세스의 CPU 처리 시간을 따로 고려하지 않기 때문에 매우 단순하고 공평한 방법이다. 하지만 CPU 처리 시간이 긴 프로세스가 앞에 올 경우 콘보이 효과가 발생한다.


### 우선순위 (Priority)

- 각 프로세스 별로 우선순위가 주어지고, 우선순위에 따라 CPU 할당
- 우선순위가 같을 경우 FCFS 적용
- 설정, 자원 상황등에 따른 우선순위를 선정해 주요/긴급 프로세스에 대한 우선처리 가능

### 기한부 (Deadline)

- 작업들이 명시된 기한 내에 완료되도록 계획한다.
- deadline은 작업의 요구사항이나 제약사항에 따라 결정된다.
- 만약 deadline을 넘어서 작업이 완료되면, 해당 작업은 tardiness(지연) 혹은 deadline miss(마감 기한 미스)로 간주되며, 이는 작업의 품질이나 서비스 수준에 부정적인 영향을 미칠 수 있다.





> CPU 스케줄러는 운영체제에서 프로세스들이 CPU를 사용하기 위해 경쟁하는 상황에서 어떤 프로세스에 CPU를 할당할지 결정하는 역할을 하는 부분입니다.
> 종류는 FCFS, SJF, SRT, Priority Scheduling, Round Robin이 있습니다.


---
# 스레싱(Thrashing)에 대한 설명

## Thrashing

스레싱(Thrashing)은 컴퓨터 운영 체제에서 발생하는 현상으로, 시스템이 페이지 부재율의 증가로 과도한 페이지 교체(또는 스왑 아웃 및 스왑 인) 작업이 일어나 실제 프로세스 처리를 수행하는 데 거의 시간을 할애하지 못하는 상태(CPU 이용률이 떨어지는 현상)를 말한다. 

다시 말해 프로세스를 처리하는 시간보다 메모리에 적재되지 못한 페이지로 인하여 페이지 교체에 드는 시간이 증가하게 되고 그로 인해 CPU 이용률이 떨어지게 되는 것

CPU 이용율이 낮다는 것은 메모리에 올라와 있는 프로세스의 수가 너무 적어 준비 큐가 비는 경우가 발생한다. 그래서 CPU 이용율이 낮으면 운영체제는 메모리에 올라가는 프로세스의 수를 늘리게 된다. 여기서 **메모리에 동시에 올라가 있는 프로세스의 수**를 **다중 프로그래밍의 정도(MPD: Multi-Programming Degree)** 라고 부릅니다.

위처럼 CPU 이용율이 낮으면 MDP를 높이게 된다. 그러나 과도하게 MPD가 높아지면 각 프로세스에 할당된 자원이 부족해지고 페이지 부재가 증가하게 된다. 디스크 IO가 늘어나며 스레싱이 발생할 수 있다.

![[Pasted image 20240319122438.png]]
> 페이지 부재(Page Fault)는 프로그램이 실행되는 동안 필요한 페이지가 메모리에 없어서 발생하는 상황을 말한다.
> 페이지 부재가 발생하면 시스템 성능에 영향을 미칠 수 있다. 디스크 I/O 작업이 발생하고 이에 따른 지연이 발생하기 때문에 프로세스의 실행 속도가 저하될 수 있다. 따라서 페이지 부재를 최소화하기 위해 메모리 관리 정책을 효율적으로 구현하는 것이 중요하다.

## 스레싱 방지 알고리즘

### 워킹셋 알고리즘

프로세스가 일정 시간동안 집중적으로 특정 주소 영역을 참조하는 경향이 있는데, 이 참조되는 페이지들의 집합을 지역성 집합이라고 한다. 
집중 참조 페이지들을 동시에 메모리에 올릴 수 있도록 보장하는 것을 워킹셋 알고리즘이라 한다.

워킹셋은 한꺼번에 메모리에 올라가야 하는 페이지들의 집합이다. 워킹셋 알고리즘은 프로세스의 워킹셋을 구성하는 페이지들이 한꺼번에 올라갈 메모리 공간이 있을 때만 동작한다. 그렇지 않으면 기존 메모리에 존재하는 페이지를 디스크로 Swap Out 시켜 공간을 확보한다. 이러한 방법으로 MPD를 조절하고 스레싱을 방지하게 된다.

### 페이지 부재 빈도 알고리즘

프로세스의 페이지 부재율을 주기적으로 조사하고 이 값에 근거해서 각 프로세스에 할당할 메모리 양을 동적으로 조절하는 알고리즘이다.
시스템이 미리 정해놓은 상한선과 하한선을 넘을 경우 운영체제가 메모리에 올라가 있는 프로세스의 수를 조절하게 된다.

![[Pasted image 20240319143130.png]]



---
# 캐시 메모리를 사용하는 이유

## Cache Memory

캐시 메모리(Cache Memory)는 CPU와 주기억장치(램) 사이에 위치하는 속도가 매우 빠른 임시 저장소이다. 이 메모리는 자주 사용되는 데이터 저장하여, CPU가 더 빠르게 접근할 수 있도록 돕는다. 

캐시 메모리의 **주요 목적**은 시스템의 성능을 향상시키는 것이며, 이는 CPU가 매번 느린 주기억장치 대신 캐시에서 데이터를 빠르게 읽거나 쓸 수 있도록 하여 성능을 향상시킬 수 있다.

캐시 메모리는 메인 메모리에서 자주 사용하는 프로그램과 데이터를 저장해두어 속도를 빠르게 한다. 속도를 빠르게 하기 위해서 CPU가 어떤 데이터를 원하는지 어느정도 예측할 수 있어야 한다. 캐시 메모리가 크기가 작기 때문에 CPU가 이후에 참조할 정보가 캐시 메모리에 어느 정도 들어있는지에 따라 캐시의 성능이 결정된다.

# CPU 적중률을 높이기 위해 어떤 원리를 사용하는지

이를 위해 캐시의 지역성(Locality)를 이용한다.

## Cache Locality

캐시의 지역성(Cache Locality)이란, 데이터에 대한 접근이 시간적 혹은 공간적으로 가깝게 발생하는 것을 말한다. 
캐시의 적중률(Hit Rate)을 극대화하여 캐시가 효율적으로 동작하기 위해 사용되는 성질이다.
지역성의 전제조건은 '프로그램은 모든 코드나 데이터를 균등하게 접근하지 않는다'는 특성을 기본으로 한다.

캐시의 지역성은 공간 지역성, 시간 지역성으로 나뉜다.
### 공간 지역성 (Spatial Locality)

최근에 사용했던 데이터와 인접한 데이터가 참조될 가능성이 높다는 특성

### 시간 지역성 (Temporal Locality)

최근에 사용했던 데이터가 잠시 후 재참조될 가능성이 높다는 특성


공간 지역성은 배열을 예로 들 수 있다. A[0], A[1]과 같은 연속 접근의 경우 그다음 원소들에 접근할 가능성이 높다. 

시간 지역성은 for, while 같은 반복문을 예로 들 수 있다. 특정 부분을 반복해서 접근하기 때문에 다시 참조할 확률이 높다.

## 캐시 크기 늘리기

캐시 크기를 늘리면 더 많은 데이터를 가져올 수 있다. But 캐시는 가격이 매우 비싸다.

---
# 메모리 단편화

메모리 단편화는 RAM 에서 메모리 공간이 작은 조각으로 나뉘어져 사용가능한 메모리가 충분히 존재하지만 할당이 불가능한 상태이다.

### 메모리 단편화로 일어날 수 있는 현상

- 총 메모리 공간은 충분하지만 실제 사용이 불가능할 수 있다.
- 실제 사용 가능한 공간이 줄어들어 시스템 성능 저하를 일으킬 수 있다.
	- 실제 사용할 수 있는 공간을 찾는 과정 필요
	- 잦은 페이지 교체
	- Swapping

---
# 외부 단편화와 내부 단편화

메모리 단편화는 외부, 내부 단편화로 구분이 가능하다.

## 내부 단편화

메모리를 할당할 때 프로세스가 필요한 양보다 더 큰 메모리가 할당되어 프로세스에서 사용하는 메모리 공간이 낭비되는 상황이다.

![[Pasted image 20240322142547.png]]

ex) 메모장을 켰을 때 OS가 10kb 할당해줬는데 7kb만 사용하고 있을 때. 내부 단편화가 3kb 만큼 생긴다.

## 외부 단편화

메모리가 할당 및 해제 작업의 반복으로 작은 메모리가 중간중간 존재하게 된다. 이때 중간에 생긴 사용하지 않는 메모리가 많이 존재해서 총 메모리 공간은 충분하지만 실제 할당이 불가능한 상황이다.

![[Pasted image 20240322142834.png]]

ex) 남은 공간은 12kb 이지만 7kb 프로세스를 분할하여 넣을 수 없으므로 할당할 수 없게 된다.

## 메모리 파편화 문제 해결 방법

- 압축, 통합, 페이징, 세그먼테이션 방식

### 압축 (Compaction)

메모리 공간을 재배치하여 단편화로 인해 분산되어 있는 메모리 공간들을 하나로 합치는 기법

![[Pasted image 20240322150045.png]]

압축 기법은 동적 할당이라는 비싼 작업을 이용하는 방법으로 자주 사용할 수 있는 방법은 아니다.

### 통합 (Coalescing)

단편화로 인해 분산된 메모리 공간들을 인접해 있는 것끼리 통합시켜 큰 메모리 공간으로 합치는 기법

- 압축은 **재배치**되는 것, 통합은 **인접 공간끼리 통합**된다는 것에서 차이가 있다.

### 페이징 (Paging)

- 가상 메모리 사용, 외부 단편화 해결, 내부 단편화 존재

![[Pasted image 20240322154338.png]]

- 페이지 : 가상메모리를 같은 크기의 블록으로 나눈 것
- 프레임 : RAM을 페이지와 같은 크기로 나눈 것

페이징 기법이란 사용하지 않는 프레임을 페이지에 옮기고, 필요한 메모리를 페이지 단위로 프레임에 옮기는 기법이다. 

- 페이지와 프레임을 대응시키기 위해 페이지 매핑 과정이 필요하기 때문에 페이징 테이블을 만든다.
- 연속적이지 않은 공간도 활용할 수 있기 때문에 외부 단편화 문제를 해결할 수 있다.
- 페이지 단위를 알맞게 꽉채워 쓰는게 아니므로 내부 단편화 문제는 여전히 있다.
- 페이지 단위를 작게하면 내부 단편화 문제도 해결할 수 있지만, 페이지 매핑 과정이 많아져 오히려 효율이 떨어질 수 있다.

**페이지 크기를 결정하는 기준**
- 내부 단편화
- Page-in, Page-out 시간 - I/O Overhead
- 페이지 테이블 크기
- 메모리 해상도(Memory resolution) - 필요한 내용만 메모리에 담을 수 있는 정도
- 페이지 부재(Page Fault) 발생 확률

**페이지 크기가 작을수록 좋은 기준**
- 내부 단편화 : 페이지가 작을수록 버려지는 메모리의 내부 공간(내부 단편화)도 작아진다.
- 메모리 해상도 : 페이지가 크면 불필요한 영역까지 함께 적재될 수밖에 없다. 반대로 적을수록 필요한 부분만 메모리에 적재되는 정밀도가 증가한다.

**페이지 크기가 클수록 좋은 기준**
- Page-in, Page-out 시간 : I/O 시간은 대부분 디스크의 헤드 이동 시간이다. 페이지가 클수록 한 번 이동해서 많은 데이터를 읽어올 수 있는 장점이 있다.
- Page Table Size : 페이지 크기가 클수록 테이블의 row 수는 적어지게 된다.
- Page Fault 발생 확률 : 페이지가 클수록 하나의 페이지 내에 많은 내용을 담고있기 때문에 페이지 재사용 가능성이 높아지게 되고 메모리 지역성에 의해 페이지 부재도 덜 발생하게 된다.

### 세그먼테이션 (Segmentation)

- 가상 메모리 사용, 내부 단편화 해결, 외부 단편화 존재

![[Pasted image 20240322153256.png]]

- 가상 메모리를 서로 크기가 다른 논리적 단위인 세그먼트로 분할해서 메모리를 할당하여 실제 메모리 주소로 변환하게 하는 방법
- 각 세그먼트는 연속적인 공간에 저장되어 있다.
- 세그먼트들의 크기가 다르기 때문에 미리 분할해둘 수 없고, 메모리에 적재될 때 빈 공간을 찾아 할당하는 기법이다.
- 매핑을 위해 세그먼트 테이블이 필요하다. (각 세그먼트의 시작 주소, 크기 정보 수록)
- 프로세스가 필요한 메모리만큼 할당해주기 때문에 내부 단편화는 일어나지 않으나, 여전히 중간에 프로세스가 메모리를 해제하면 생기는 구멍. 즉, 외부 단편화 문제는 여전히 존재한다.


# 가상 메모리

프로세스 전체가 메모리에 올라가지 않더라도 실행이 가능하도록 하는 기법

- 애플리케이션이 실행될 때, 실행에 필요한 일부분만 메모리에 올라가며 애플리케이션의 나머지는 디스크에 남게 된다. 즉, 디스크가 RAM의 보조기억장치처럼 작동하는 것이다.
	- 결국 빠르고 작은 기억장치(RAM)을 크고 느린 기억장치(디스크)와 병합하여, 하나의 크고 빠른 기억장치(가상 메모리)처럼 동작하게 하는 것이다.
- 가상 메모리를 구현하기 위해 컴퓨터가 특수 메모리 관리 하드웨어 **MMU (Memory Management Unit)** 를 갖추고 있어야 한다. 

- MMU는 가상 주소를 물리 주소로 변환하고, 메모리를 보호하는 기능을 수행
- MMU를 사용하면, CPU가 각 메모리에 접근하기 이전에 메모리 주소 번역 작업 수행
- 그러나 메모리를 일일이 가상 주소에서 물리적 주소로 번역하게 되면 작업 부하가 너무 높아지므로, MMU는 RAM을 여러 페이지로 나누어 각 페이지를 하나의 독립된 항목으로 처리
- 페이지 및 주소 번역 정보를 기억하는 작업이 가상 메모리를 구현하는 데 있어 결정적인 절차이다.



# 요구 페이징

운영체제에서 사용하는 가상 메모리 관리 기법 중 하나이다.

프로세스가 실행되는 동안 **필요한 페이지**만 메모리에 올리고, 필요하지 않은 페이지는 디스크에 저장하여 메모리를 절약하는 방법이다. 

### 장점

1. 필요한 페이지만 메모리에 적재하기 때문에 메모리 사용량이 감소한다.
2. 프로세스 전체를 메모리에 올리는 데 소요되는 입출력 오버헤드가 감소한다.
3. 사용되지 않는 주소 영역에 대한 입출력이 줄어 응답시간이 줄어든다.
4. 시스템이 더 많은 프로세스를 수용할 수 있게 해준다.
5. 물리적 메모리의 제약을 벗어날 수 있다.

### 페이지 부재 (page fault)

메모리에 페이지가 존재하는지 구별하기 위해 유효-무효 비트(valid-invalid bit)를 두어 각 페이지가 존재하는지 표시한다.

**유효/무효 비트** : 해당 비트가 유효하면 메모리에 있음을 의미하고, 무효하면 메모리에 없음을 의미한다.

CPU가 무효 비트로 표시된 페이지에 엑세스하는 상황을 페이지 부재라고 한다.

# 페이지 교체 알고리즘

page fault 가 발생하면 요청된 페이지를 디스크에서 메모리로 가져온다. 이때 물리적 메모리 공간이 부족한 상황이 발생할 수 있다. 이럴 때 메모리에 올라와 있는 페이지를 디스크로 옮겨 메모리 공간을 확보해야 한다. 이를 **페이지 교체** 라 한다.

페이지 교체 알고리즘의 핵심은 메모리에서 앞으로 사용할 가능성이 적은 페이지를 대상 페이지로 선정하여 페이지 부재를 줄이고 시스템의 성능을 향상하는 것이다.

## FIFO 페이지 교체 알고리즘

시간상으로 가장 먼저 들어온 페이지를 스왑 아웃 시킨다.

페이지 부재는 F, 원하는 페이지가 메모리에 있을 경우 S로 표시

![[Pasted image 20240323150528.png]]

페이지 교체 알고리즘은 앞으로 사용하지 않을 페이지를 스왑 아웃 시키는 것이 중요한데, FIFO는 성능이 떨어진다.

## 최적 페이지 교체 알고리즘 (Optimal)

앞으로 사용할 페이지를 미리 살펴보고 페이지 교체 선정 시점부터 사용 시점까지 가장 멀리 있는 페이지. 즉, 앞으로 사용하지 않을 가능성이 높은 페이지를 대상 페이지로 선정한다.

![[Pasted image 20240323150755.png]]

최적 페이지 교체 알고리즘은 이상적인 방법이고 실제로 미래에 접근할 페이지를 알고 구현할 수 없다.
이를 해결하기 위해 최적 페이지 교체 알고리즘에 근접하는 알고리즘이 나왔다.
과거의 데이터를 바탕으로 미래의 접근 패턴을 추정하기 때문에 최적 근접 알고리즘 (Optimal Approximation Algorithm)이라고 부른다.

## LRU 페이지 교체 알고리즘 (Least Recently Used)

메모리에 올라온 후 가장 오랫동안 사용되지 않은 페이지를 스왑 아웃 시키는 알고리즘이다.

가장 오래 사용되지 않은 페이지 판단 방법에 따라 구현 방식은 여러가지이다.

### 페이지 접근 시간에 기반한 구현

접근 시각을 기록하여 판단하는 방식이다.
메모리를 추가적으로 사용해서 각 페이지에 접근한 시각을 기록한다.

### 카운터 기반 구현

카운터를 사용하여 구현하는 방법이다. 즉, 메모리를 추가적으로 사용해서 페이지에 접근한 시간이 아닌, 몇 번째로 접근했는지를 기록하는 것이다.

페이지 접근 시각, 카운터 모두 메모리 공간이 추가로 들어간다는 단점이 존재한다.

### 참조 비트 시프트 방식

각 페이지에 일정 크기의 참조 비트를 만들어 사용하는 것이다. 참조 비트의 초기값은 0이며 페이지에 접근할 때마다 1로 바뀐다. 또한, 참조 비트는 주기적으로 한 칸씩 오른쪽 시프트(>>) 된다.

![[Pasted image 20240323152144.png]]

![[Pasted image 20240323152354.png]]

참조 비트 시프트 방식은 가장 오랫동안 접근하지 않은 페이지를 교체하는 것으로 위 그림에서 C가 대상 페이지가 된다.

LRU 페이지 교체 알고리즘은 **메모리가 추가적으로 필요하여 낭비되는 메모리 공간이 많아진다는 단점**이 있다.

## LFU 페이지 교체 알고리즘 (Least Frequently Used)

페이지가 몇 번 사용되었는지를 기준으로 대상 페이지를 선정한다. 
현재 프레임에 있는 페이지마다 사용된 횟수를 세어 횟수가 가장 적은 페이지를 스왑 아웃 시킨다.

사용 빈도가 같을 경우 FIFO, LRU, Random 과 같은 추가 기준 알고리즘을 사용하여 대상 페이지를 선정한다.

알고리즘 성능은 LRU와 비슷하지만, 페이지 접근 횟수를 표시하는데 LRU 알고리즘과 마찬가지로 메모리의 추가 공간이 필요하므로 낭비되는 메모리 공간이 많다는 단점이 존재한다.

## NUR 페이지 교체 알고리즘 (Not Used Recently)

LRU, LFU 페이지 교체 알고리즘과 성능이 거의 비슷하면서 불필요한 메모리 공강 낭비 문제를 해결한 알고리즘이다.

NUR는 **PTE의 접근(참조) 비트, 변경 비트**를 이용해서 대상 페이지를 선정한다.

**접근 비트** 는 페이지에 접근. 즉, 읽기나 실행 연산이 되면 1이 되고, 
**변경 비트** 는 페이지가 변경. 즉, 쓰기나 추가 연산이 되면 1이 된다.
따라서, 초기 상태 (0, 0) 에서 시작하여 접근이 발생하면 (1, 0) , 변경이 발생하면 (0, 1) 이 된다. 둘 다 발생하면 (1, 1) 이 된다.

![[Pasted image 20240323154854.png]]

NUR 알고리즘 성능은 LRU, LFU와 비슷하지만 낭비되는 메모리 공간이 없다는 장점이 있다.
하지만 실제로는 LRU 알고리즘이 많이 사용된다.

NUR 을 채택하지 않는 이유는 각 페이지의 참조를 유지 및 업데이트하고 비트를 수정하기 위해 추가적인 하드웨어 지원이나 소프트웨어 오버헤드가 필요하기 때문이다.

반면, LRU는 LFU, NUR보다 구현이 비교적 쉽고, 오버헤드가 적기 때문에 많이 사용되는 알고리즘이다.


## FIFO 변형 알고리즘

FIFO 페이지 교체 알고리즘을 기본으로하고 페이지 접근할 때마다 순서에 변화를 주어 성능을 향상한 알고리즘이다.

### 2차 기회 페이지 교체 알고리즘(Second Chance)

특정 페이지에 접근하여 페이지 부재 없이 성공할 경우 해당 페이지를 큐의 맨 뒤로 이동하여 대상 페이지에서 제외, 즉 성공할 기회를 한 번 더 주는 알고리즘이다. 큐의 맨 뒤로 옮기면 대상 페이지로 선정될 확률이 줄어드는 특징을 이용했다.

![[Pasted image 20240323155756.png]]

4-5 번 시점에서 기존 FIFO는  BCD -> BCD 그대로 인데, 2차 기회 페이지 교체 알고리즘은 BCD -> CDB 로 B를 맨뒤로 보낸다.

성능은 LRU, LFU, NUR 알고리즘보다 약간 낮고, FIFO보다 약간 높다고 한다. 하지만, 큐를 유지하는 비용이 높고, 페이지가 성공하면 큐의 중간에 있는 값을 맨 뒤로 이동시키는 작업이 추가 된다는 단점이 있다.

### 시계 알고리즘

원형 큐를 이용하여 대상 페이지를 선정하는 알고리즘이다.

![[Pasted image 20240323160333.png]]

시계 알고리즘에서는 대상 페이지를 가리키는 포인터를 사용한다. 이 포인터가 큐의 맨 아래로 내려가면 다시 큐의 처음을 가리킨다.


![[Pasted image 20240323160324.png]]

시계 알고리즘은 PTE의 접근(참조) 비트를 사용하는데, 메모리에 있는 페이지가 성공적으로 참조되면 0에서 1로 변경된다.

4번 시점처럼 메모리가 꽉 찼을 경우 대상 페이지를 가리키는 포인터가 스왑 인이 되었을 경우 밑으로 이동한다. 이때, 9번 시점처럼 참조 비트가 1인 페이지는 0으로 바꿔놓고 건너뛴다.

Linux, Windows, macOS 와 같은 운영체제들은 LRU, 시계 알고리즘을 혼용해서 페이지 교체를 한다고 한다. 가장 오랫동안 사용되지 않는 페이지를 찾는 데 효과적이고, 최근에 참조된 페이지를 보존하는 데 효과적인 두 알고리즘의 장점을 조합하여 페이지 교체의 효율성을 높였다.




---
# 동시성과 병렬성 차이

## 동시성 (Concurrency)

> 동시성은 하나의 시스템이 여러 작업을 동시에 처리하는 것처럼 보이게 하는 것이다.

실질적으로는 한 번에 하나의 작업만을 처리한다.

![[Pasted image 20240326152656.png]]

- 동시성은 대개 스레드, 코루틴, 비동기 프로그래밍 등의 방법을 사용하여 구현된다.
- 동시성은 여러 작업을 번갈아가며 처리하므로 작업이 빠르게 완료될 수 있다.

비슷한 개념으로 멀티 태스킹이 있다.

### 멀티태스킹 (Multitasking)

멀티태스킹은 운영체제에서 하나의 컴퓨터에서 여러 개의 프로그램이 동시에 실행될 수 있는 기능을 말한다. 멀티태스킹은 일반적으로 CPU 시간을 분할하여 여러 프로그램이 동시에 실행되는 것처럼 보이도록 한다.

- 멀티태스킹은 주로 운영체제에서 제공되며, 여러 개의 프로세스나 스레드를 동시에 실행하고 관리하는 방법으로 구현된다. 
- 일반적으로 운영체제는 CPU 시간을 조절하고 프로세스나 스레드 간의 우선 순위를 관리하여, 여러 작업이 동시에 실행될 수 있도록 한다.

### 동시성 vs 멀티태스킹

동시성과 멀티태스킹은 비슷한 개념으로 사용되지만 약간의 차이가 있다.

- 멀티태스킹
	- 하나의 시스템에서 여러 개의 작업(task)을 동시에 실행하는 것을 말한다.
	- 운영체제에서는 이를 위해 여러 개의 프로세스 또는 스레드를 생성하여 동시에 실행한다.
	- 이러한 멀티태스킹 방식은 하나의 CPU를 사용하여 여러 작업을 처리하므로 시스템 자원의 효율성이 높아진다.

- 동시성
	- 하나의 작업 내에서 여러 개의 서브태스크를 동시에 처리하는 것을 말한다.
	- 이는 여러 개의 스레드를 생성하여 하나의 작업을 분할하여 처리하거나, 비동기적으로 여러 개의 작업을 처리하는 등을 말한다. 
	- 동시성은 멀티태스킹과 마찬가지로 동시에 작업을 수행하지만, 작업이 독립적이지 않고 서로 의존성을 가지는 경우에 적합하다.

프로세스가 스레드를 포함하듯, 멀티태스킹은 프로세스의 동작을, 동시성은 스레드의 동작을 의미할 수 있다.

## 병렬성 (Parallelism)

> 병렬성은 여러 작업을 실제로 동시에 처리하는 것이다.

여러 CPU 또는 코어를 사용하여 여러 작업을 병렬로 처리할 수 있다.

- 병렬로 처리할 작업들은 병렬처리기에서 실행되며, 각각의 작업은 별도의 프로세스나 스레드에서 실행된다. 이러한 병렬처리기는 여러 개의 CPU 또는 CPU 코어가 있어서, 각각의 작업이 서로 다른 CPU 또는 CPU 코어에서 병렬적으로 사용된다.

- 병렬성은 멀티코어 컴퓨터에서 많이 사용되며, 실행 시간을 줄이거나 처리량을 늘리는 데에 사용된다.
- 예를 들어, 병렬처리를 사용하여 이미지나 비디오를 인코딩하거나, 대규모 데이터를 처리하는 등의 작업을 효율적으로 처리할 수 있다.

## 동시성과 병렬성 차이

동시성과 병렬성은 비슷해 보이지만 완전 다른 개념이다.

병렬성을 여러 작업이 동시에 실행되는 것이지만, 이러한 작업들은 각각 독립적으로 실행되며 서로 영향을 주지 않는다.
반면 동시성은 서로 다른 작업들이 서로 영향을 주면서 동시에 실행되는 것처럼 보인다.

| 구분      | 동시성                                         | 병렬성                                        |
| ------- | ------------------------------------------- | ------------------------------------------ |
| 개념      | **동시에 처리하는 것처럼** 보이게 하는 것                   | 여러 작업을 실제로 동시에 처리하는 것                      |
| 사용 코어 수 | 싱글 코어                                       | 멀티 코어                                      |
| 동작 방식   | **싱글 코어**에서 멀티 쓰레드(Multi thread)를 동작 시키는 방식 | **멀티 코어**에서 멀티 쓰레드(Multi thread)를 동작시키는 방식 |
| 개념적 차이  | 논리적인 개념                                     | 물리적인 개념                                    |


---
# 커널

커널은 운영체제 중 항상 메모리에 올라가 있는 운영체제의 핵심 부분으로, 하드웨어와 응용프로그램 사이에서 인터페이스를 제공하는 역할을 하며 컴퓨터 자원들을 관리하는 역할을 한다.

즉, 커널은 인터페이스로써 응용 프로그램 수행에 필요한 여러가지 서비스를 제공하고, 여러가지 하드웨어(CPU, 메모리) 등의 리소스를 관리하는 역할을 한다.

다만 커널은 항상 컴퓨터 자원들만 바라보고 있기 때문에 사용자와의 상호작용은 지원하지 않는다. 사용자와의 직접적인 상호작용을 위해 프로그램을 제공하게 되는데, 대표적으로 쉘(Shell)이라는 명령어 해석기 등이 있다.

# 유저 모드와 커널 모드
## 이중 모드 (Dual mode)

운영체제는 사용자가 실행하는 응용 프로그램이 하드웨어 자원에 직접 접근하는 것을 방지하여 자원을 보호한다. 만약 응용 프로그램이 직접 CPU, 메모리, 하드 디스크 등을 조작할 수 있게 되면 무질서하게 관리될 것이고, 응용 프로그램이 조금만 실수해도 컴퓨터 전체에 악영향을 끼칠 수 있다. 

그래서 운영체제는 응용프로그램이 자원에 접근하려고 할 때 오직 자신을 통해서 접근하도록 하여 자원을 보호한다. (문지기 역할)

![[Pasted image 20240326164017.png]]

이러한 운영체제의 문지기 역할은 이중 모드로써 구현된다.
이중 모드란 CPU가 명령어를 실행하는 모드를 크게 사용자 모드와 커널 모드로 구분하는 방식이다. CPU는 명령어를 사용자 모드로써 실행할 수 있고, 커널 모드로써 실행할 수 있다.


## 유저 모드

사용자 모드는 운영체제 서비스를 제공받을 수 없는 실행 모드이다. 사용자 모드로 실행중인 CPU는 입출력 명령어와 같이 하드웨어 자원에 접근하는 명령어를 실행할 수 없다. 그래서 사용자 모드로 실행되는 일반적인 응용 프로그램은 자원에 접근할 수 없다.

## 커널 모드

커널 모드는 운영체제 서비스를 제공받을 수 있는 실행 모드이다. CPU가 커널 모드로 명령어를 실행하면 자원에 접근하는 명령어를 비롯한 모든 명령어를 실행할 수 있다. 운영체제는 커널 모드로 실행되기 때문에 자원에 접근할 수 있다.

사용자 모드로 실행되는 프로그램이 자원에 접근하는 운영체제 서비스를 제공받으려면 운영체제에 요청을 보내 커널 모드로 전환해야 한다. 이때 운영체제 서비스를 제공받기 위한 요청을 시스템 콜이라고 한다. 



# Blocking/Non-Blocking, 동기와 비동기

- **Blocking** : 호출된 함수가 자신의 작업을 모두 마칠 때까지 제어권을 호출자에게 반환하지 않는다. 즉, 작업이 완료될 때까지 기다린다.
- **Non-Blocking** : 호출된 함수가 작업 완료 여부와 상관없이 즉시 제어권을 호출자에게 반환한다. 호출자는 작업이 완료되었는지 여부를 주기적으로 확인하거나, 완료 통지를 받는다.
- **동기 (Synchronous)** : 작업들이 순서대로 실행되며, 한 작업이 완료된 후에 다음 작업이 시작된다. 동기 방식은 구현이 간단하고 직관적이지만, 특정 작업이 긴 시간이 소요될 경우 전체 시스템의 성능에 영향을 줄 수 있다.
- **비동기 (Asynchronous)** : 하나의 작업이 완료될 때까지 기다리지 않고, 다음 작업을 바로 시작할 수 있다. 비동기 방식은 동시에 여러 작업을 처리할 수 있어 효율적이지만, 작업의 완료 순서를 관리하거나 오류를 처리하는 등의 복잡성이 증가한다.

## 차이점

- Blocking / Non-Blocking 은 작업이 **호출자에게 얼마나 빨리 제어권을 반환하는지에** 초점을 둔다. 즉, 작업 진행 상태를 기다리느냐 마냐의 문제이다.
- Synchronous / Asynchronous 는 작업들 사이의 의존성과 실행 순서에 대한 처리 방식 차이를 다룬다. 동기는 **순차적인 실행**, 비동기는 **병렬 실행**을 가능하게 한다.


## Sync/Async, Blocking/Non-Blocking 조합

프로그램 아키텍처에서는 이 두 개념이 조합되어 사용된다.

- Sync Blocking (동기 + 블로킹)
- Async Blocking (비동기 + 블로킹)
- Sync Non-Blocking (동기 + 논블로킹) 
- Async Non-Blocking (비동기 + 논블로킹)

![[Pasted image 20240327173328.png]]

### Sync Blocking (동기 + 블로킹)

다른 작업이 진행되는 동안 자신의 작업을 처리하지 않고 (Blocking), 다른 작업이 완료되면 순차적으로 처리하는(Syncronous) 방식이다. 다른 작업 결과가 자신의 작업에 영향을 주는 경우에 활용할 수 있다.

작은 데이터를 처리하거나 파일 하나를 읽고 쓰는 경우에는 Sync Blocking 방식이 간단하고 직관적일 수 있다. 하지만 작업량이 많거나 시간이 오래걸리는 작업을 처리해야 하는 경우 작업이 끝날 때까지 다른 작업을 처리하지 못하므로 비효율적이다. 이 경우에는 Async Non-Blocking 방식을 이용하여 작업을 처리하는 것이 좋다.


### Async Non-Blocking (비동기 + 논블로킹)

다른 작업이 진행되는 동안에도 자신의 작업을 처리하고 (Non Blocking), 다른 작업의 결과를 바로 처리하지 않아 작업 순서가 지켜지지 않는(Async) 방식이다.
다른 작업의 결과가 자신의 작업에 영향을 주지 않는 경우에 활용할 수 있다.

작업량이 많거나 시간이 오래 걸리는 작업을 처리해야 하는 경우에 적합하다.
예를 들어, 대용량 데이터를 처리하거나 많은 요청을 처리하는 서비스에서는 Async Non-Blocking 방식을 사용하여 한 작업이 처리되는 동안 다른 작업을 처리할 수 있으므로 전체 처리 시간을 줄일 수 있어 애플리케이션 처리 성능을 향상시킬 수 있다.


### Sync Non-Blocking (동기 + 논블로킹) 

다른 작업이 진행되는 동안에도 자신의 작업을 처리하고 (Non-Blocking), 다른 작업의 결과를 바로 처리하여 작업을 순서대로 수행하는 (Sync)방식이다.

예시 1) 게임에서 맵을 이동할 때, 우선 맵 데이터를 모두 다운 받아야 할 것이다. 그동안 화면에는 로딩 화면이 뜬다. 로딩바가 채워지는 프로그램이 수행되고 있는 것이다.
즉, 제어권은 여전히 나한테 있어 화면에 퍼센트가 표시되는 것이다. 그리고 맵이 어느정도 로드 됐는지 끊임없이 조회한다.
자신의 작업을 계속하고 있지만 다른 작업과의 동기를 위해 계속해서 다른 작업이 끝났는지 조회하는 것이다.


# 인터럽트

인터럽트는 CPU가 프로그램을 실행하고 있을 때, 입출력 하드웨어 등의 장치나 예외 상황이 발생하여 처리가 필요할 경우에 프로그램을 잠시 중단하고 발생한 일을 처리한 후, 다시 실행중인 작업으로 돌아오는 것을 말한다.

인터럽트가 발생하면 CPU에서는 즉각적으로 인터럽트 처리를 위해 커널 코드를 커널 모드에서 실행한다.
## 인터럽트 종류

- 전원(power)에 문제가 생겼을 때
- I/O 작업이 완료 됐을 때
- 시간이 다 됐을 때

프로그램 레벨에서 발생하는 인터럽트 (트랩)
- 0 으로 나눴을 때
- 잘못된 메모리 공간에 접근을 시도할 때 


# 시스템 콜

프로그램이 OS 커널이 제공하는 서비스를 이용하고 싶을 때 시스템 콜을 통해 실행한다.

시스템 콜이 발생하면 해당 커널 코드가 커널 모드에서 실행된다.

## 시스템 콜 종류

- 프로세스 / 스레드 관련
- 파일 I/O 관련
- 소켓 관련
- 디바이스 관련
- 프로세스 통신 관련


하드웨어 혹은 시스템 관련 기능은 어떤 프로그램이라도 반드시 시스템 콜을 통해서만 사용이 가능하다.
하지만 보통 개발할 때 직접 OS 시스템 콜을 사용하지 않는데, 그럼에도 파일 I/O, 네트워크 I/O, 프로세스/스레드 관련 작업을 해왔다.
프로그래밍 언어로 시스템 콜 작업이 가능했던 이유는 프로그래밍 언어들이 시스템 콜을 Wrapping 하여 간접적으로 사용할 수 있도록 제공했기 때문이다.

```java
java.lang.Thread Class

Thread thread = new Thread();
thread.start();

---

public synchronized void start() {
	...
	boolean started = false;
	try {
		start0();
		started = true;
	}
}

// native 는 보통 운영체제 
// JNI 인터페이스를 통해 OS system call 호출 
private native void start0();
```


# 
- 운영체제란?
- 실행파일 생성 과정
- 프로그램 실행과정
    - 명령어사이클
- 캐시란?
- 캐시라인
- 메모리구조
    - 전역변수와 정적변수의 차이
    - Heap, Stack 비교
    - 가시성(visibility)과 원자성(atomocity)
    - 유효주소, 주소지정방식
    - 메모리 할당 알고리즘
- 프로세스, 스레드
    - 멀티 프로세스와 멀티 스레드
    - 스레드와 프로세스 콘텍스트 스위치 차이 이유
    - 프로세스 스케줄러에 대해
    - CPU 스케줄러
- 스레싱이란?
- IPC란?
- Race Condition?
- User, Kernel 스레드 차이
- 가상 메모리
    - 메모리 단편화
- 페이징기법
- 세그먼테이션 기법ㅏ
- 메모리풀
- 페이지 교체 알고리즘
- MMU란?
- TLB란?
- 페이지 부재시 절차
- 인터럽트란?
    - System Call
- 프로세스 제어 명령
- fork(), vfork() 차이
- 시스템콜과 서브루틴 차이
- 블록킹, 논블록킹
- 동기, 비동기
- 동기 IO 처리과정 : 프로세스 A가 디스크에서 어떤 데이터 읽어올때 상황
- 입출력 처리방식
    - DMA
    - Cycle Stealing
- 데드락
    - 발생조건
    - 회피기법
    - 해결경험
- PCB(Process Control Block) 이란?
- Spin Lock 이란?
- 동기화 종류
- Critical Section?
- Mutex란?
- Semaphore란?
- Mutex, Semaphore 차이
- Monitor란?
- Thread-safe 란?
    - Reentrant란?
- 인터럽트, 폴링 개념 및 차이
- Fault Tolerance 란?
    - 작동절차